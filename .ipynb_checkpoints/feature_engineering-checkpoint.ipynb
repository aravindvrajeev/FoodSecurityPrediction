{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Standard imports\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "# We do this to ignore several specific Pandas warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Malawi Data\n",
    "url_mw = 'https://raw.githubusercontent.com/zhou100/FoodSecurityPrediction/master/data/clean/dataset/mw_dataset_cluster.csv'\n",
    "mw_village = pd.read_csv(url_mw)\n",
    "mw_village = mw_village.drop('year',axis=1)\n",
    "\n",
    "\n",
    "# Tanzania Data \n",
    "url_tz = 'https://raw.githubusercontent.com/zhou100/FoodSecurityPrediction/master/data/clean/dataset/tz_dataset_cluster.csv'\n",
    "tz_village = pd.read_csv(url_tz)\n",
    "\n",
    "# Uganda Data \n",
    "url_ug = 'https://raw.githubusercontent.com/zhou100/FoodSecurityPrediction/master/data/clean/dataset/ug_dataset_cluster.csv'\n",
    "ug_village = pd.read_csv(url_ug)\n",
    "\n",
    "\n",
    "ug_village = ug_village.replace([np.inf, -np.inf], np.nan)\n",
    "ug_village = ug_village.dropna()\n",
    "\n",
    "# Malawi household Data\n",
    "url_mw_hh = 'https://raw.githubusercontent.com/zhou100/FoodSecurityPrediction/master/data/clean/household/mw_hh_aggregate.csv'\n",
    "mw_hh = pd.read_csv(url_mw_hh)\n",
    "\n",
    " \n",
    "# Tanzania household Data \n",
    "url_tz_hh = 'https://raw.githubusercontent.com/zhou100/FoodSecurityPrediction/master/data/clean/household/tz_hh_aggregate.csv'\n",
    "tz_hh = pd.read_csv(url_tz_hh)\n",
    "tz_hh=tz_hh.rename(index=str, columns={\"clusterid\": \"ea_id\"})\n",
    "\n",
    "# Uganda household Data \n",
    "url_ug_hh = 'https://raw.githubusercontent.com/zhou100/FoodSecurityPrediction/master/data/clean/household/ug_hh_aggregate.csv'\n",
    "ug_hh = pd.read_csv(url_ug_hh)\n",
    "ug_hh = ug_hh[[\"HHID\",\"FCS\",\"FS_month\",\"FS_year\",\"ea_id\"]] \n",
    " \n",
    "\n",
    "# check for any missing values (should return false)\n",
    "print(ug_village.isnull().values.any())\n",
    "print(tz_village.isnull().values.any())\n",
    "print(mw_village.isnull().values.any())\n",
    "\n",
    "\n",
    "# check for any missing values (should return false)\n",
    "print(ug_hh.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate different outcome variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_fs_three(df, measure):\n",
    "    \n",
    "    '''\n",
    "    helper function to categorize continous food measure based on given cutoffs to cut into three categories \n",
    "    '''\n",
    "    \n",
    "    if ( measure == 'FCS'):\n",
    "        labels = [2,1,0]\n",
    "        bins= [-1,28,42,200]\n",
    "        \n",
    "    elif (measure == 'rCSI'):   \n",
    "        labels = [0,1,2]\n",
    "        bins= [-1,4,17,50]\n",
    "        \n",
    "    categorized = measure + '_3_category'\n",
    "    \n",
    "    df[categorized] = pd.cut( x= df[measure], bins = bins, labels = labels)\n",
    "    \n",
    "    return df \n",
    "\n",
    "def categorize_fs_binary(df, measure):\n",
    "    \n",
    "    '''\n",
    "    helper function to categorize continous food measure based on binary cutoffs\n",
    "    '''\n",
    "    if ( measure == 'FCS'):\n",
    "        labels = [1,0]\n",
    "        bins= [-1,42,200]\n",
    "        \n",
    "    elif (measure == 'rCSI'):   \n",
    "        labels = [0,1]\n",
    "        bins= [-1,4,50]\n",
    "        \n",
    "    categorized = measure + '_binary_category'\n",
    "    \n",
    "    df[categorized] = pd.cut( x= df[measure], bins = bins, labels = labels)\n",
    "    \n",
    "    return df \n",
    "\n",
    "def category_percent(df,measure,category):\n",
    "    '''\n",
    "    helper function to \n",
    "    calculate the percent of a certain food security category in a given country \n",
    "    '''\n",
    "    \n",
    "    categorized = measure + '_3_category'\n",
    "    \n",
    "    category_name = '_low_' if category==2 else '_mid_'\n",
    "\n",
    "    count_name = measure + category_name + 'count'\n",
    "    \n",
    "    percent_name = measure + category_name + 'percent'\n",
    "    \n",
    "    df_count = df[df[categorized]==category].groupby(['ea_id','FS_year']).count().reset_index()[['ea_id','FS_year',categorized]]\n",
    "    df_count.columns=['ea_id','FS_year',count_name]\n",
    "    \n",
    "    df_total= df.groupby(['ea_id','FS_year']).count().reset_index()[['ea_id','FS_year',measure]]\n",
    "    df_total.columns=['ea_id','FS_year','num_hh']\n",
    "  \n",
    "    df_percent = pd.merge(df_total, df_count, on=['ea_id','FS_year'])\n",
    "    df_percent[percent_name] = round(df_percent[count_name]/df_percent['num_hh'],3)\n",
    "    \n",
    "    df_percent = df_percent.drop(columns= ['num_hh'])\n",
    "        \n",
    "    return df_percent\n",
    "\n",
    "def village_percent(df_village,df_hh,measure): \n",
    "    '''\n",
    "    calculate and merge the percent numbers into village level dfs\n",
    "    '''\n",
    "    \n",
    "    df_hh=categorize_fs_three(df_hh,measure=measure)\n",
    "    \n",
    "        \n",
    "    df_category_mid = category_percent(df_hh,measure=measure,category=1)\n",
    "    df_category_low = category_percent(df_hh,measure=measure,category=2)\n",
    "\n",
    "    \n",
    "    df_village = pd.merge(df_village, df_category_mid, on=['ea_id','FS_year'])\n",
    "    df_village = pd.merge(df_village, df_category_low, on=['ea_id','FS_year'])\n",
    "    \n",
    "    df_village[measure+'_mid+low'] = df_village[measure+'_low_percent'] + df_village[measure +'_mid_percent']\n",
    "    \n",
    "    df_village=categorize_fs_three(df_village,measure=measure)\n",
    "    df_village=categorize_fs_binary(df_village,measure=measure)\n",
    "\n",
    "       \n",
    "\n",
    "    return df_village"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_village_list = [mw_village,tz_village,ug_village]\n",
    "df_hh_list = [mw_hh,tz_hh,ug_hh]\n",
    "measure_list = ['FCS','rCSI']\n",
    "\n",
    "for index in range(len(df_village_list)):\n",
    "      \n",
    "    df_village_list[index] = village_percent(df_village_list[index],df_hh_list[index],measure='FCS') \n",
    "\n",
    "for index in range(len(df_village_list)-1):\n",
    "      \n",
    "    df_village_list[index] = village_percent(df_village_list[index],df_hh_list[index],measure='rCSI') \n",
    "\n",
    "mw_village = df_village_list[0]\n",
    "tz_village = df_village_list[1]\n",
    "ug_village = df_village_list[2]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ea_id', 'FS_year', 'FNID', 'FCS', 'HDDS', 'rCSI', 'rural', 'FS_month',\n",
       "       'region2', 'region3', 'region4', 'region5', 'region6', 'region7',\n",
       "       'region8', 'region9', 'region10', 'region11', 'region12', 'region13',\n",
       "       'region14', 'region15', 'region16', 'region17', 'region18', 'region19',\n",
       "       'region20', 'region21', 'region51', 'region52', 'region53', 'region54',\n",
       "       'region55', 'lat_modified', 'lon_modified', 'dist_road',\n",
       "       'dist_popcenter', 'percent_ag', 'nutri_severe_constraint',\n",
       "       'nutri_moderate_constraint', 'nutri_reten_severe_constraint',\n",
       "       'dummy_terrain_rough', 'head_age', 'female_head', 'asset_index',\n",
       "       'Cellphone', 'num_cell', 'floor_dirt_sand_dung', 'roof_not_natural',\n",
       "       'roof_iron', 'clust_maize_price', 'clust_rice_price',\n",
       "       'clust_bean_mktthin', 'clust_maize_mktthin', 'clust_rice_mktthin',\n",
       "       'lhz_maize_price', 'lhz_rice_price', 'lhz_bean_mktthin',\n",
       "       'lhz_maize_mktthin', 'lhz_rice_mktthin', 'raincytot', 'day1rain',\n",
       "       'maxdaysnorain', 'lhz_day1rain', 'gdd', 'tmean', 'lhz_raincytot',\n",
       "       'lhz_maxdaysnorain', 'heatdays', 'floodmax', 'lhz_floodmax',\n",
       "       'FCS_mid_count', 'FCS_mid_percent', 'FCS_low_count', 'FCS_low_percent',\n",
       "       'FCS_mid+low', 'FCS_3_category', 'FCS_binary_category',\n",
       "       'rCSI_mid_count', 'rCSI_mid_percent', 'rCSI_low_count',\n",
       "       'rCSI_low_percent', 'rCSI_mid+low', 'rCSI_3_category',\n",
       "       'rCSI_binary_category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tz_village.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_split(country, df):\n",
    "    if country == \"mw\":\n",
    "        test_year = 2015\n",
    "    elif country == \"tz\":\n",
    "        test_year = 2013\n",
    "    elif country == \"ug\":\n",
    "        test_year = 2011\n",
    "    \n",
    "    df_test = df[df['FS_year']>test_year]\n",
    "    df_train = df[df['FS_year']<test_year]\n",
    "\n",
    "    return df_test,df_train \n",
    "          \n",
    "\n",
    "def separate_y(country,df_test,df_train):\n",
    "    if country != \"ug\":\n",
    "        labels = ['FCS', 'rCSI']\n",
    "        category_labels = ['FCS_3_category', 'FCS_binary_category', 'rCSI_3_category','rCSI_binary_category']\n",
    "        percent_labels = ['FCS_mid_count', 'FCS_mid_percent', 'FCS_low_count', 'FCS_low_percent','FCS_mid+low','rCSI_mid_count','rCSI_mid_percent', 'rCSI_low_count','rCSI_low_percent', 'rCSI_mid+low']\n",
    "               \n",
    "    elif country == \"ug\":      \n",
    "        labels = ['FCS']\n",
    "        category_labels = ['FCS_3_category', 'FCS_binary_category']\n",
    "        percent_labels = ['FCS_mid_count', 'FCS_mid_percent', 'FCS_low_count', 'FCS_low_percent','FCS_mid+low']\n",
    "    \n",
    "    id_vars = [\"ea_id\",\"FS_year\",\"lat_modified\",\"lon_modified\",\"HDDS\",\"FNID\"]\n",
    "    X_test = df_test.drop(labels+category_labels+percent_labels+id_vars,  axis=1)\n",
    "    X_train = df_train.drop(labels+category_labels+percent_labels+id_vars,  axis=1)\n",
    "    y_train_category = df_train[category_labels]\n",
    "    y_test_category = df_test[category_labels]\n",
    "    y_train_percent = df_train[percent_labels]\n",
    "    y_test_percent = df_test[percent_labels]\n",
    "\n",
    "    return X_train,X_test,y_train_category,y_test_category,y_train_percent,y_test_percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def lrCLF(X_train,y_train,X_test, y_test):\n",
    "    '''logistic'''\n",
    "    lr_clf = LogisticRegression(random_state=66, solver='lbfgs',\n",
    "                              multi_class='multinomial')\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = lr_clf.predict(X_test)\n",
    "    # y_prob = lr_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    return y_pred,y_test\n",
    "    # return y_prob,y_test\n",
    "    \n",
    "\n",
    "def treeCLF(X_train,y_train,X_test, y_test):\n",
    "    '''Tree'''\n",
    "    # Define tree classifier\n",
    "    tree_clf = DecisionTreeClassifier(random_state=66)\n",
    "    \n",
    "    max_depth = [int(x) for x in np.linspace(3, 20, num = 10)]\n",
    "    max_features = [int(x) for x in np.linspace(3, 20, num = 10)]\n",
    "    # n_estimators = [int(x) for x in np.linspace(1, 10, num = 5)]\n",
    "\n",
    "    random_grid = {#'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth\n",
    "                   #'min_samples_split': min_samples_split\n",
    "                   #'min_samples_leaf': min_samples_leaf}\n",
    "                   #'bootstrap': bootstrap\n",
    "                    }\n",
    "    \n",
    "    tree_random = RandomizedSearchCV(estimator = tree_clf, param_distributions = random_grid,\n",
    "                                    n_iter = 30, cv = 3, verbose=2, random_state=666, n_jobs = -1)\n",
    "\n",
    "\n",
    "    tree_random.fit( X_train, y_train)\n",
    "\n",
    "    y_pred = tree_random.predict(X_test)\n",
    "    \n",
    "    # y_prob = tree_random.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    \n",
    "    return y_pred,y_test\n",
    "\n",
    "def rfCLF(X_train,y_train,X_test, y_test):\n",
    "    '''rfc'''\n",
    "    rf_clf = RandomForestClassifier(max_features='auto', n_estimators = 500,min_samples_split=10,warm_start=True)\n",
    "\n",
    "    # Define rfc classifier\n",
    "    max_depth = [int(x) for x in np.linspace(12, 30, num = 10)]\n",
    "    max_features = [int(x) for x in np.linspace(8, 20, num = 10)]\n",
    "\n",
    "    random_grid = {#'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth\n",
    "                   #'min_samples_split': min_samples_split\n",
    "                   #'min_samples_leaf': min_samples_leaf}\n",
    "                   #'bootstrap': bootstrap\n",
    "                    }\n",
    "    \n",
    "    rf_random = RandomizedSearchCV(estimator = rf_clf, \n",
    "            param_distributions = random_grid,\n",
    "            refit ='recall', \n",
    "            n_iter = 30, cv = 3, verbose=2, random_state=666, \n",
    "            n_jobs = -1)\n",
    "\n",
    "\n",
    "    # Fit the random search model\n",
    "    rf_random.fit(X_train, y_train)\n",
    "  \n",
    "    y_pred = rf_random.predict(X_test)\n",
    " \n",
    "    #y_prob = rf_random.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    return y_pred,y_test\n",
    "\n",
    "def xgbCLF(X_train,y_train,X_test, y_test):\n",
    "    '''XGB'''\n",
    "\n",
    "    # fit model on  training data\n",
    "    XGB_clf = XGBClassifier(silent=False, \n",
    "                          scale_pos_weight=1,\n",
    "                          learning_rate=0.3,  \n",
    "                          colsample_bytree = 0.4,\n",
    "                          subsample = 0.8,\n",
    "                          objective='binary:logistic', \n",
    "                          #objective='multi:softmax', \n",
    "                          #num_class=14,\n",
    "                          n_estimators=100, \n",
    "                          reg_alpha = 0.3,\n",
    "                          max_depth=5, \n",
    "                          gamma=10)\n",
    "    \n",
    "    max_depth = [int(x) for x in np.linspace(2, 5, num = 10)]\n",
    "    max_features = [int(x) for x in np.linspace(3, 20, num = 10)]\n",
    "\n",
    "    random_grid = {#'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth\n",
    "                   #'min_samples_split': min_samples_split\n",
    "                   #'min_samples_leaf': min_samples_leaf}\n",
    "                   #'bootstrap': bootstrap\n",
    "                    }\n",
    "    XGB_random = RandomizedSearchCV(estimator = XGB_clf, param_distributions = random_grid,\n",
    "                                    n_iter = 30, cv = 3, verbose=2, random_state=666, n_jobs = -1)\n",
    "    # Fit the random search model\n",
    "    XGB_random.fit(X_train, y_train)\n",
    "    y_pred = XGB_random.predict(X_test)\n",
    "    \n",
    "    # y_prob = XGB_random.predict_proba(X_test.values)[:, 1]\n",
    "\n",
    "    return y_pred,y_test\n",
    "\n",
    "def pre_rec_f1_support_minority(y_pred,y_test):\n",
    "    \n",
    "    if len(precision_recall_fscore_support(y_true=y_test,y_pred=y_pred)[3])==3:\n",
    "        precision = precision_recall_fscore_support(y_true=y_test,y_pred=y_pred)[0][2]\n",
    "        recall = precision_recall_fscore_support(y_true=y_test,y_pred=y_pred)[1][2]\n",
    "        fscore = precision_recall_fscore_support(y_true=y_test,y_pred=y_pred)[2][2]\n",
    "        support = precision_recall_fscore_support(y_true=y_test,y_pred=y_pred)[3][2]\n",
    "\n",
    "    elif len(precision_recall_fscore_support(y_true=y_test,y_pred=y_pred)[3])==2:\n",
    " #   precision = precision_recall_fscore_support(y_true=y_test,y_pred=y_pred)[0][1]\n",
    " #   recall = precision_recall_fscore_support(y_true=y_test,y_pred=y_pred)[1][1]\n",
    " #   fscore = precision_recall_fscore_support(y_true=y_test,y_pred=y_pred)[2][1]\n",
    " #   support = precision_recall_fscore_support(y_true=y_test,y_pred=y_pred)[3][1]\n",
    "\n",
    "        precision = np.nan\n",
    "        recall = np.nan\n",
    "        fscore = np.nan\n",
    "        support = np.nan\n",
    "\n",
    "    accuracy = accuracy_score(y_pred,y_test)\n",
    "\n",
    "    return precision,recall,fscore,support,accuracy    \n",
    " \n",
    "def pre_rec_f1_support_mid(y_pred,y_test):\n",
    "\n",
    "    precision = precision_recall_fscore_support(y_true=y_test,y_pred=y_pred)[0][1]\n",
    "    recall = precision_recall_fscore_support(y_true=y_test,y_pred=y_pred)[1][1]\n",
    "    fscore = precision_recall_fscore_support(y_true=y_test,y_pred=y_pred)[2][1]\n",
    "    support = precision_recall_fscore_support(y_true=y_test,y_pred=y_pred)[3][1]\n",
    "    accuracy = accuracy_score(y_pred,y_test)\n",
    "\n",
    "    return precision,recall,fscore,support,accuracy        \n",
    "\n",
    "def metrics_3_category(country,df,measure,model):\n",
    "    \n",
    "    category = measure+'_3_category'\n",
    "    classifier = model\n",
    "    \n",
    "    df_test,df_train = year_split(country, df)\n",
    "    X_train,X_test,y_train_category,y_test_category,y_train_percent,y_test_percent = separate_y(country,df_test,df_train)\n",
    "    y_pred,y_test  = classifier(X_train,y_train_category[category],X_test,y_test_category[category])\n",
    "    \n",
    "    precision,recall,fscore,support,accuracy = pre_rec_f1_support_minority(y_pred,y_test)\n",
    "    \n",
    "    result_row = [[country,measure, model.__doc__, precision,recall,fscore,support,accuracy]]\n",
    "    result_df = pd.DataFrame(result_row,columns = ['country','measure','model', 'precision','recall','fscore','support','accuracy']) \n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  90 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  90 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  75 out of  90 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  90 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:   17.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:   18.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:   13.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:   12.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:   13.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  90 | elapsed:    2.3s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:    2.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:    1.1s finished\n"
     ]
    }
   ],
   "source": [
    "# create a table of results for the third category\n",
    "\n",
    "third_category_results = pd.DataFrame(columns = ['country','measure','model', 'precision','recall','fscore','support','accuracy']) \n",
    "\n",
    "model_list= [lrCLF,treeCLF,rfCLF,xgbCLF]\n",
    "country_list = [\"mw\",\"tz\",\"ug\"]\n",
    "measure_list = [\"FCS\",\"rCSI\"]\n",
    "\n",
    "\n",
    "for model in model_list:\n",
    "    for country in country_list:\n",
    "        for measure in measure_list:\n",
    "            if (measure==\"rCSI\") & (country==\"ug\"):\n",
    "                pass\n",
    "            else:\n",
    "                if country==\"mw\":\n",
    "                    df = mw_village\n",
    "                elif country == \"ug\":\n",
    "                    df = ug_village\n",
    "                elif country == \"tz\":\n",
    "                    df = tz_village\n",
    "            \n",
    "                third_category_results = third_category_results.append(metrics_3_category(country,df,measure,model),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `recall_score()` not found.\n"
     ]
    }
   ],
   "source": [
    "third_category_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
